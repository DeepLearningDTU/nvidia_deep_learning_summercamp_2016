{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn.datasets\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "import lasagne\n",
    "\n",
    "def plot_decision_boundary(pred_func, X, y):\n",
    "    #from https://github.com/dennybritz/nn-from-scratch/blob/master/nn-from-scratch.ipynb\n",
    "    # Set min and max values and give it some padding\n",
    "    x_min, x_max = X[:, 0].min() - .5, X[:, 0].max() + .5\n",
    "    y_min, y_max = X[:, 1].min() - .5, X[:, 1].max() + .5\n",
    "    \n",
    "    h = 0.01\n",
    "    # Generate a grid of points with distance h between them\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
    "    \n",
    "    yy = yy.astype('float32')\n",
    "    xx = xx.astype('float32')\n",
    "    # Predict the function value for the whole gid\n",
    "    Z = pred_func(np.c_[xx.ravel(), yy.ravel()])[:,0]\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    # Plot the contour and training examples\n",
    "    plt.figure()\n",
    "    plt.contourf(xx, yy, Z, cmap=plt.cm.RdBu)\n",
    "    plt.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.Spectral)\n",
    "\n",
    "def onehot(t, num_classes):\n",
    "    out = np.zeros((t.shape[0], num_classes))\n",
    "    for row, col in enumerate(t):\n",
    "        out[row, col] = 1\n",
    "    return out\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural networks 101\n",
    "In this notebook you will implement a simple neural network in Lasagne utilizing the automatic differentiation engine of Theano. We assume that you are already familiar with backpropation (if not please see [Andrej Karpathy](http://cs.stanford.edu/people/karpathy/) or [Michal Nielsen](http://neuralnetworksanddeeplearning.com/chap2.html).\n",
    "We'll not spend much time on how Theano works, but you can refer to [this short tutorial](http://nbviewer.jupyter.org/github/craffel/theano-tutorial/blob/master/Theano%20Tutorial.ipynb) if you are interested.\n",
    "\n",
    "Additionally, for the ambitious people we have previously made an assignment where you will implement both the forward and backpropagation in a neural network by hand, https://github.com/DTU-deeplearning/day1-NN/blob/master/exercises_1.ipynb \n",
    "\n",
    "In this exercise we'll start right away by defining logistic regression model in Lasagne/Theano. Some details of Theano can be a bit confusing, however you'll pick them up when you worked with it for some time. For now you should pay most attention to the highlevel network construction in Lasagne. We'll initially start with a simple 2-D and 2-class classification problem where the class decision boundary can be visualized. Initially we show that logistic regression can only separate classes linearly. Adding a Non-linear hidden layer to the algorithm permits nonlinear class separation. If time permits we'll continue on to implement a fully conencted neural network to classify the (in)famous MNIST dataset consisting of images of hand written digits. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem \n",
    "We'll initally demonstrate the that MLPs can classify non-linear problems whereas simple logistic regression cannot. For ease of visualization and computationl speed we initially experiment on the simple 2D half-moon dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Generate a dataset and plot it\n",
    "np.random.seed(0)\n",
    "num_samples = 300\n",
    "\n",
    "X, y = sklearn.datasets.make_moons(num_samples, noise=0.20)\n",
    "\n",
    "X_tr = X[:100].astype('float32')\n",
    "X_val = X[100:200].astype('float32')\n",
    "X_te = X[200:].astype('float32')\n",
    "\n",
    "y_tr = y[:100].astype('int32')\n",
    "y_val = y[100:200].astype('int32')\n",
    "y_te = y[100:200].astype('int32')\n",
    "\n",
    "plt.scatter(X_tr[:,0], X_tr[:,1], s=40, c=y_tr, cmap=plt.cm.BuGn)\n",
    "\n",
    "print X.shape, y.shape\n",
    "\n",
    "num_features = X_tr.shape[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# From Logistic Regression to \"Deep Learning\" in Lasagne\n",
    "The building block in lasagne is the Layer. To get started the most important layers are the DenseLayer and the InputLayer. \n",
    "\n",
    "The [InputLayer](http://lasagne.readthedocs.io/en/latest/modules/layers/input.html) is a \"special\" layer which lets you input data to the network. The InputLayer is initialized with a tuple specifying the shape of the input data. Note that it is common to provide ``None`` for the first dimension which allows you to vary the batch size at runtime. \n",
    "\n",
    "The [DenseLayer](http://lasagne.readthedocs.io/en/latest/modules/layers/dense.html) implements the computation: \n",
    "\n",
    "$$y = xW + b$$\n",
    "\n",
    "where $x$ is the layer input, $y$ is the layer output and $\\{W, b\\}$ are the layer parameters. The DenseLayer is initialized with a pointer to the previous layer, the desired number of units in the layer and the nonlinerity. \n",
    "\n",
    "\n",
    "\n",
    "1. Given the input $x$ and the number of units in the layer lasagne infers the shapes of $W$ and $b$ and keep track of the layer parameters.\n",
    "2. Setup the computation $y = xW + b$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from lasagne.updates import sgd\n",
    "from lasagne.nonlinearities import leaky_rectify, softmax, tanh, elu\n",
    "from lasagne.layers import InputLayer, DenseLayer\n",
    "\n",
    "\n",
    "#MODEL SPECIFICATION\n",
    "l_in = InputLayer(shape=(None, num_features))\n",
    "#INSERT HIDDEL LAYER HERE\n",
    "#l = DenseLayer(incoming=l,.....\n",
    "l_out = DenseLayer(incoming=l_in, num_units=2, nonlinearity=softmax, name='outputlayer') \n",
    "#We use two output units since we have two classes. the softmax function ensures that the the class probabilities sum to 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After we have built the network we can use lasagnes helper functions to \n",
    "\n",
    "1. Build the computation graph: __[lasagne.layers.get_output](http://lasagne.readthedocs.io/en/latest/modules/layers/helper.html#lasagne.layers.get_output)__ . The ``deterministic`` flag tells lasagne if we are in training mode or evaluation mode. When you build more complicated networks this is very important to remember!\n",
    "2. Collect the network parameters: __[lasagne.layers.get_all_params](http://lasagne.readthedocs.io/en/latest/modules/layers/helper.html#lasagne.layers.get_all_params)__\n",
    "\n",
    "Note that all the helper functions are called with the output layer as argument or a list of outputlayers if you have multiple output layers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sym_x = T.matrix('X') # a symbolic variable taking on the value of a input batch.\n",
    "sym_t = T.ivector('target') # a symbolic variable taking on the value of the target batch.\n",
    "\n",
    "\n",
    "# Get network output\n",
    "train_out = lasagne.layers.get_output(l_out, {l_in: sym_x}, deterministic=False)\n",
    "eval_out = lasagne.layers.get_output(l_out, {l_in: sym_x}, deterministic=True)\n",
    "\n",
    "\n",
    "# Get list of all trainable parameters in the network.\n",
    "all_params = lasagne.layers.get_all_params(l_out, trainable=True)\n",
    "\n",
    "# print shapes of all the paramters in the network.\n",
    "for p in all_params:\n",
    "    print p, p.get_value().shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``train_out`` will be a symbolic variable representing the network output. Using ``train_out`` we  can define the [crossentropy error](http://deeplearning.net/software/theano/library/tensor/nnet/nnet.html#tensor.nnet.categorical_crossentropy) used for training the network.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cost_train = T.nnet.categorical_crossentropy(train_out, sym_t).mean()\n",
    "cost_eval = T.nnet.categorical_crossentropy(eval_out, sym_t).mean()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we train a neural network we update the parameters in direction of the negative gradient w.r.t the cost.\n",
    "We can use ``T.grad`` to get the gradients for all parameters in the network w.r.t ``cost_train``.\n",
    "\n",
    "Finally we can use __[lasagne.updates.sgd](http://lasagne.readthedocs.io/en/latest/modules/updates.html#lasagne.updates.sgd)__ to calculate the stochastic gradient descent (SGD) update rule for each paramter in the network. ``updates`` is a dictionary of the parameter update rules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Let Theano do its magic and get all the gradients we need for training\n",
    "all_grads = T.grad(cost_train, all_params)\n",
    "\n",
    "# Set the update function for parameters \n",
    "# you might wan't to experiment with more advanded update schemes like rmsprob, adadelta etc.\n",
    "updates = lasagne.updates.sgd(all_grads, all_params, learning_rate=1.0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final step is to compile Theano functions for the network. For theano functions we need to specify which inputs the function should take. For our network that is ``sym_x`` which is the input data and ``sym_t`` which is the targets. Secondly we need to specify which outputs we want the network to return. In our case that is the crossentropy cost and the network output.\n",
    "\n",
    "When we compile ``f_train`` we additionally gives the updates dictionary as input. This tell Theano to update the network parameters with the update rules everytime we call ``f_train``. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f_eval = theano.function(inputs=[sym_x, sym_t],\n",
    "                         outputs=[cost_eval, eval_out])\n",
    "\n",
    "f_train = theano.function(inputs=[sym_x, sym_t],\n",
    "                          outputs=[cost_train, eval_out],\n",
    "                          updates=updates)\n",
    "\n",
    "\n",
    "\n",
    "#now you have three functions. \n",
    "# f_train(X,y) -> cost, y_pred which will update the parameters using backprop each time you call it, only use this on the training data!\n",
    "# f_test(X,y) -> cost, y_pred which only calculates the forward pass\n",
    "\n",
    "\n",
    "#This us just a helper function for plotting the decision boundaries between the two classes\n",
    "f_pred = theano.function(inputs=[sym_x],\n",
    "                         outputs=eval_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot_decision_boundary(lambda x: f_pred(x), X_val,y_val)\n",
    "plt.title(\"Untrained Classifier\")\n",
    "\n",
    "num_epochs = 1000\n",
    "\n",
    "train_cost, val_cost = [],[]\n",
    "for e in range(num_epochs):\n",
    "    out = f_train(X_tr,y_tr)\n",
    "    #out = [cost, y_pred]\n",
    "    train_cost += [out[0]]\n",
    "    \n",
    "    out = f_eval(X_val,y_val)\n",
    "    val_cost += [out[0]]\n",
    "\n",
    "    if e % 100 == 0:\n",
    "        print \"Epoch %i, Train Cost: %0.3f\\tVal Cost: %0.3f\"%(e, train_cost[-1],val_cost[-1])\n",
    "    \n",
    "    \n",
    "out = f_eval(X_te,y_te)\n",
    "test_cost = out[0]\n",
    "print \"\\nTest Cost: %0.3f\"%(test_cost)\n",
    "\n",
    "plot_decision_boundary(lambda x: f_pred(x), X_te, y_te)\n",
    "plt.title(\"Trained Classifier\")\n",
    "\n",
    "epoch = np.arange(len(train_cost))\n",
    "plt.figure()\n",
    "plt.plot(epoch,train_cost,'r',epoch,val_cost,'b')\n",
    "plt.legend(['Train Loss','Val Loss'])\n",
    "plt.xlabel('Updates'), plt.ylabel('Loss')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignments Half Moon\n",
    "\n",
    " 1) A linear logistic classifier is only able to create a linear decision boundary. Change the Logistic classifier into a (non-linear) Neural network by inserting a dense hidden layer between the input and output layers of the model\n",
    " \n",
    " 2) Experiment with multiple hidden layers or more / less hidden units. What happens to the decision bondary?\n",
    " \n",
    " 3) Overfitting: When increasing the number of hidden layers / units the neural network will fit the training data better by creating a highly nonlinear decision boundary. If the model is to complex it will often generalize poorly to new data (validation and test set). Can you obseve this from the training and validation errors? \n",
    " \n",
    " 3) We used the vanilla stocastic gradient descent algorithm for parameter updates. This is usually slow to converge and more sophisticated pseudo-second-order methods usually works better. Try changing the optimizer to [adam or adamax](http://lasagne.readthedocs.io/en/latest/modules/updates.html) (lasagne.updates.adam, lasagne.updates.adamax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optional:  MNIST dataset\n",
    "The MNIST dataset consists of 70,000 images of handwritten digits from 0-9. The dataset is split into a 50,000 images training set, 10,000 images validation set and 10,000 images test set. The images are 28x28 pixels, where each pixel represents a normalised value between 0-255 (0=black and 255=white).\n",
    "\n",
    "### Primer for tomorrow\n",
    "We use a feedforward neural network to classify the 28x28 mnist images. ``num_features`` is therefore 28x28=784.\n",
    "That is we represent each image as a vector. The ordering of the pixels in the vector does not matter, so we could permuate all images using the same permuataion and still get the same performance. (Your are of course encouraged to try this using ``numpy.random.permutation`` to get a random permutation :)). This task is therefore called the _permutation invariant_ MNIST. Obviously this throws away a lot of structure in the data. Tomorrow we'll fix this with the convolutional neural network wich encodes prior knowledgde about data that has either spatial or temporal structure.  \n",
    "\n",
    "\n",
    "\n",
    "## MNIST\n",
    "First let's load the MNIST dataset and plot a few examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#To speed up training we'll only work on a subset of the data\n",
    "data = np.load('mnist.npz')\n",
    "num_classes = 10\n",
    "x_train = data['X_train'][:1000].astype('float32')\n",
    "targets_train = data['y_train'][:1000].astype('int32')\n",
    "\n",
    "x_valid = data['X_valid'][:500].astype('float32')\n",
    "targets_valid = data['y_valid'][:500].astype('int32')\n",
    "\n",
    "x_test = data['X_test'][:500].astype('float32')\n",
    "targets_test = data['y_test'][:500].astype('int32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#plot a few MNIST examples\n",
    "idx = 0\n",
    "canvas = np.zeros((28*10, 10*28))\n",
    "for i in range(10):\n",
    "    for j in range(10):\n",
    "        canvas[i*28:(i+1)*28, j*28:(j+1)*28] = x_train[idx].reshape((28, 28))\n",
    "        idx += 1\n",
    "plt.figure(figsize=(7, 7))\n",
    "plt.axis('off')\n",
    "plt.imshow(canvas, cmap='gray')\n",
    "plt.title('MNIST handwritten digits')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#defined the model\n",
    "num_class = 10\n",
    "num_features = x_train.shape[1]\n",
    "\n",
    "l_in = InputLayer(shape=(None,num_features))\n",
    "l_hid = DenseLayer(incoming=l_in, num_units=500, nonlinearity=elu)\n",
    "l_out = DenseLayer(incoming=l_hid, num_units=num_class, nonlinearity=softmax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sym_x = T.matrix('sym_x') # a symbolic variable taking on the value of a input batch.\n",
    "sym_t = T.ivector('sym_t') # a symbolic variable taking on the value of the target batch.\n",
    "\n",
    "# Get network output\n",
    "train_out = lasagne.layers.get_output(l_out, sym_x, deterministic=False)\n",
    "eval_out = lasagne.layers.get_output(l_out, sym_x, deterministic=True)\n",
    "\n",
    "\n",
    "# Get list of all trainable parameters in the network.\n",
    "all_params = lasagne.layers.get_all_params(l_out, trainable=True)\n",
    "\n",
    "cost = T.nnet.categorical_crossentropy(train_out+1e-8, sym_t).mean()\n",
    "# Let Theano do its magic and get all the gradients we need for training\n",
    "all_grads = T.grad(cost, all_params)\n",
    "\n",
    "\n",
    "# Set the update function for parameters \n",
    "# you might wan't to experiment with more advanded update schemes like rmsprob, adadelta etc.\n",
    "updates = lasagne.updates.sgd(all_grads, all_params, learning_rate=0.1)\n",
    "\n",
    "\n",
    "f_eval = theano.function([sym_x],\n",
    "                     eval_out, on_unused_input='warn')\n",
    "\n",
    "f_train = theano.function([sym_x, sym_t],\n",
    "                          [cost],\n",
    "                          updates=updates, on_unused_input='warn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Test the forward pass\n",
    "x = np.random.normal(0,1, (45, 28*28)).astype('float32') #dummy data\n",
    "\n",
    "model = lasagne.layers.get_output(l_out, sym_x)\n",
    "out = model.eval({sym_x:x}) #this could also include mask etc if used\n",
    "print \"l_out\", out.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build the training loop.\n",
    "We train the network by calculating the gradient w.r.t the cost function and update the parameters in direction of the negative gradient. \n",
    "\n",
    "\n",
    "When training neural network you always use mini batches. Instead of calculating the average gradient using the entire dataset you approximate the gradient using a mini-batch of typically 16 to 256 samples. The paramters are updated after each mini batch. Networks converges much faster using minibatches because the paramters are updated more often.\n",
    "\n",
    "We build a loop that iterates over the training data. Remember that the parameters are updated each time ``f_train`` is called."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from confusionmatrix import ConfusionMatrix\n",
    "batch_size = 100\n",
    "num_epochs = 100\n",
    "learning_rate = 0.1\n",
    "num_samples_train = x_train.shape[0]\n",
    "num_batches_train = num_samples_train // batch_size\n",
    "num_samples_valid = x_valid.shape[0]\n",
    "num_batches_valid = num_samples_valid // batch_size\n",
    "\n",
    "train_acc, train_loss = [], []\n",
    "valid_acc, valid_loss = [], []\n",
    "test_acc, test_loss = [], []\n",
    "cur_loss = 0\n",
    "loss = []\n",
    "for epoch in range(num_epochs):\n",
    "    #Forward->Backprob->Update params\n",
    "    cur_loss = 0\n",
    "    for i in range(num_batches_train):\n",
    "        idx = range(i*batch_size, (i+1)*batch_size)\n",
    "        x_batch = x_train[idx]\n",
    "        target_batch = targets_train[idx]    \n",
    "        batch_loss = f_train(x_batch,target_batch) #this will do the complete backprob pass\n",
    "        cur_loss += batch_loss[0]\n",
    "    loss += [cur_loss/batch_size]\n",
    "    \n",
    "    confusion_valid = ConfusionMatrix(num_classes)\n",
    "    confusion_train = ConfusionMatrix(num_classes)\n",
    "\n",
    "    for i in range(num_batches_train):\n",
    "        idx = range(i*batch_size, (i+1)*batch_size)\n",
    "        x_batch = x_train[idx]\n",
    "        targets_batch = targets_train[idx]\n",
    "        net_out = f_eval(x_batch)   \n",
    "        preds = np.argmax(net_out, axis=-1) \n",
    "        confusion_train.batch_add(targets_batch, preds)\n",
    "\n",
    "    confusion_valid = ConfusionMatrix(num_classes)\n",
    "    for i in range(num_batches_valid):\n",
    "        idx = range(i*batch_size, (i+1)*batch_size)\n",
    "        x_batch = x_valid[idx]\n",
    "        targets_batch = targets_valid[idx]\n",
    "        net_out = f_eval(x_batch)   \n",
    "        preds = np.argmax(net_out, axis=-1) \n",
    "        \n",
    "        confusion_valid.batch_add(targets_batch, preds)\n",
    "    \n",
    "    train_acc_cur = confusion_train.accuracy()\n",
    "    valid_acc_cur = confusion_valid.accuracy()\n",
    "\n",
    "    train_acc += [train_acc_cur]\n",
    "    valid_acc += [valid_acc_cur]\n",
    "    print \"Epoch %i : Train Loss %e , Train acc %f,  Valid acc %f \" \\\n",
    "    % (epoch+1, loss[-1], train_acc_cur, valid_acc_cur)\n",
    "    \n",
    "    \n",
    "epoch = np.arange(len(train_acc))\n",
    "plt.figure()\n",
    "plt.plot(epoch,train_acc,'r',epoch,valid_acc,'b')\n",
    "plt.legend(['Train Acc','Val Acc'])\n",
    "plt.xlabel('Updates'), plt.ylabel('Acc')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#More questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Do you see overfitting? Google overfitting if you don't know how to spot it\n",
    "2. Regularization is a method to reduce overfitting. Adding noise to your network is a popular method to fight overfitting. Try using Dropout in your network. [Lasagne DropoutLayer](http://lasagne.readthedocs.io/en/latest/modules/layers/noise.html#lasagne.layers.DropoutLayer).\n",
    "3. Alternatively you can regularize your network by penalizing the L2 or L1 norm of the network parameters. [Read the docs for more info](http://lasagne.readthedocs.io/en/latest/modules/regularization.html). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
