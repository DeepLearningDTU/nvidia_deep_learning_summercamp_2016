{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn.datasets\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "import lasagne\n",
    "import math\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variational Autoencoders (VAE)\n",
    "\n",
    "In this exercise we'll implement an variational autoencoder. Very briefly an autoencoder encodes some input into a new representaiton and usually more compact representation which can be used to reconstruct the input data again. An variational autoencoder makes the furhter assumption that the compact representation is follows probabilistic distribution (usually a gaussian) which makes it possible to sample new data from a trained variational autoencoder. The \"variational\" part of the name comes from the fact that these models are training using variational inference.\n",
    "\n",
    "The mathematical details of the training can be a bit challenging however we believe that probabilistic deep learning will be an important part of future deep learning developments why we find it important to introduce the concepts.\n",
    "\n",
    "As background material we recommend reading [Tutorial on Variational Autoencoder](http://arxiv.org/abs/1606.05908). For the implementation of the model you must read the article \"Auto-Encoding Variational Bayes\", Kingma & Welling, ICLR 2014: http://arxiv.org/pdf/1312.6114v10.pdf and \"Stochastic Backpropagation and Approximate Inference in Deep Generative Models\", Rezende et al, ICML 2014:\n",
    "http://arxiv.org/pdf/1401.4082v3.pdf\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VAE crash course\n",
    "\n",
    "VAEs consist of two parts:\n",
    "\n",
    " * Encoder (also known as recognition, inference or Q-model): Maps the input data into a probabilistic latent space by calculating the mean and variance parameters of a gaussian distribution as a function of the input data x:  $q(z|x) = \\mathcal{N}(z|\\mu_\\theta(x), \\sigma_\\phi(x)I)$\n",
    " * Decoder (also known as generative or P-model): Reconstructs the input image using a sample from the latent space defined by the encoder model: $p(x|z)$\n",
    "<img src=\"VAE.png\" alt=\"Drawing\" style=\"width: 300px;\"/>\n",
    "\n",
    "\n",
    "In more mathematical details we have (this can be a bit challenging)\n",
    "\n",
    "$p(x) = \\int_z p(x|z)p(z)dz$\n",
    "\n",
    "$p(x) = \\int_z p(x|z)p(z)\\frac{q(z|x)}{q(z|x)}dz$\n",
    "\n",
    "\n",
    "$p(x) = \\int_z q(z|x) \\frac{p(x|z)p(z)}{q(z|x)}dz$\n",
    "\n",
    "\n",
    "$\\log p(x) = \\log \\int_z q(z|x) \\frac{p(x|z)p(z)}{q(z|x)}dz$\n",
    "\n",
    "$\\log p(x) \\geq  \\int_z q(z|x)\\log \\frac{p(x|z)p(z)}{q(z|x)}dz$\n",
    "\n",
    "This is know as the variational lower bound. We contiue with a bit of rewriting\n",
    "\n",
    "$\\log p(x) \\geq E_{q(z|x)} \\left[\\log \\frac{p(x|z)p(z)}{q(z|x)}\\right]$\n",
    "\n",
    "$\\log p(x) \\geq E_{q(z|x)} \\left[\\log p(x|z)\\right] - KL(q(z|x) | p(z))$\n",
    "\n",
    "Here the first term on the right hand side are the data reconstruction and the second term the Kulback-Liebler divergenve between the approximate and true posterior distributions which acts as a probabilistic regularizer.\n",
    "\n",
    "### Training a VAE \n",
    "The VAE is similar to an deterministic autoencoder except that we assume that the hidden units are following some distribution. Usually we just assume that the units are independent standard gaussian distributed.\n",
    "\n",
    "Above we defined a lower bound on the log likelihood of the data. We can train the model by pushing up the lowerbound. I'e we do gradient ascent on the lowerbound.  By using the _reparameterization trick_ we can directly backprop throug the model and uptimize the lower bound. If you are interested in the technical details you can look at the references given above.\n",
    "\n",
    "### Setting up the network\n",
    "\n",
    "We set up the network like an autoencoder except that the bottle neck layer is the __SimpleSampleLayer__ which samples the hidden units. \n",
    "\n",
    "The lower bound is calculated in the ```LogLikelihood```. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#To speed up training we'll only work on a subset of the data\n",
    "#We discretize the data to 0 and 1 in order to use it with a bernoulli observation model p(x|z) = Ber(mu(z))\n",
    "\n",
    "def bernoullisample(x):\n",
    "    return np.random.binomial(1,x,size=x.shape).astype(theano.config.floatX)\n",
    "\n",
    "\n",
    "data = np.load('mnist.npz')\n",
    "num_classes = 10\n",
    "x_train = bernoullisample(data['X_train'][:50000]).astype('float32')\n",
    "targets_train = data['y_train'][:50000].astype('int32')\n",
    "\n",
    "x_valid = bernoullisample(data['X_valid'][:500]).astype('float32')\n",
    "targets_valid = data['y_valid'][:500].astype('int32')\n",
    "\n",
    "x_test = bernoullisample(data['X_test'][:500]).astype('float32')\n",
    "targets_test = data['y_test'][:500].astype('int32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#plot a few MNIST examples\n",
    "\n",
    "def plot_samples(x,title=''):\n",
    "    idx = 0\n",
    "    canvas = np.zeros((28*10, 10*28))\n",
    "    for i in range(10):\n",
    "        for j in range(10):\n",
    "            canvas[i*28:(i+1)*28, j*28:(j+1)*28] = x[idx].reshape((28, 28))\n",
    "            idx += 1\n",
    "    plt.figure(figsize=(7, 7))\n",
    "    plt.imshow(canvas, cmap='gray')\n",
    "    plt.title(title)\n",
    "    plt.show()\n",
    "\n",
    "plot_samples(x_train[:100],title='MNIST handwritten digits')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#defined a couple of helper functions\n",
    "c = - 0.5 * math.log(2*math.pi)\n",
    "def log_bernoulli(x, p, eps=0.0):\n",
    "    p = T.clip(p, eps, 1.0 - eps)\n",
    "    return -T.nnet.binary_crossentropy(p, x)\n",
    "\n",
    "def kl_normal2_stdnormal(mean, log_var):\n",
    "    return -0.5*(1 + log_var - mean**2 - T.exp(log_var))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Construct the lasagne layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from lasagne.layers import InputLayer,DenseLayer,get_output, get_all_params\n",
    "from lasagne.nonlinearities import elu, identity, sigmoid\n",
    "from samplelayer import SimpleSampleLayer\n",
    "\n",
    "num_features = x_train.shape[-1]\n",
    "num_latent_z = 64\n",
    "\n",
    "#MODEL SPECIFICATION\n",
    "\n",
    "#ENCODER\n",
    "l_in_x = InputLayer(shape=(None, num_features))\n",
    "l_enc = DenseLayer(l_in_x, num_units=256, nonlinearity=elu)\n",
    "l_enc = DenseLayer(l_enc, num_units=256, nonlinearity=elu) \n",
    "l_muq = DenseLayer(l_enc, num_units=num_latent_z, nonlinearity=identity)     #mu(x)\n",
    "l_logvarq = DenseLayer(l_enc, num_units=num_latent_z, nonlinearity=lambda x: T.clip(x,-10,10)) #logvar(x), \n",
    "\n",
    "l_z = SimpleSampleLayer(mean=l_muq, log_var=l_logvarq) #sample a latent representation z \\sim q(z|x) = N(mu(x),logvar(x))\n",
    "\n",
    "#we split the in two parts to allow sampling from the decoder model separately\n",
    "\n",
    "#DECODER\n",
    "l_in_z = InputLayer(shape=(None, num_latent_z))\n",
    "l_dec = DenseLayer(l_in_z, num_units=256, nonlinearity=elu) \n",
    "l_dec = DenseLayer(l_dec, num_units=256, nonlinearity=elu) \n",
    "l_mux = DenseLayer(l_dec, num_units=num_features, nonlinearity=sigmoid)  #reconstruction of input using a sigmoid output since mux \\in [0,1] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "sym_x = T.matrix('x')\n",
    "sym_z = T.matrix('z')\n",
    "\n",
    "z_train, muq_train, logvarq_train = get_output([l_z,l_muq,l_logvarq],{l_in_x:sym_x},deterministic=False)\n",
    "mux_train = get_output(l_mux,{l_in_z:z_train},deterministic=False)\n",
    "\n",
    "z_eval, muq_eval, logvarq_eval = get_output([l_z,l_muq,l_logvarq],{l_in_x:sym_x},deterministic=True)\n",
    "mux_eval = get_output(l_mux,{l_in_z:z_eval},deterministic=True)\n",
    "\n",
    "mux_sample = get_output(l_mux,{l_in_z:sym_z},deterministic=True)\n",
    "\n",
    "\n",
    "\n",
    "#defined the cost function\n",
    "\n",
    "def LogLikelihood(mux,x,muq,logvarq):\n",
    "    log_px_given_z = log_bernoulli(x, mux, eps=1e-6).sum(axis=1).mean() #note that we sum the latent dimension and mean over the samples\n",
    "    KL_qp = kl_normal2_stdnormal(muq, logvarq).sum(axis=1).mean()\n",
    "    LL = log_px_given_z - KL_qp\n",
    "    return LL, log_px_given_z, KL_qp\n",
    "\n",
    "\n",
    "LL_train, logpx_train, KL_train = LogLikelihood(mux_train, sym_x, muq_train, logvarq_train)\n",
    "LL_eval, logpx_eval, KL_eval = LogLikelihood(mux_eval, sym_x, muq_eval, logvarq_eval)\n",
    "\n",
    "\n",
    "all_params = get_all_params([l_z,l_mux],trainable=True)\n",
    "\n",
    "# Let Theano do its magic and get all the gradients we need for training\n",
    "all_grads = T.grad(-LL_train, all_params)\n",
    "\n",
    "\n",
    "# Set the update function for parameters \n",
    "# you might wan't to experiment with more advanded update schemes like rmsprob, adadelta etc.\n",
    "updates = lasagne.updates.adam(all_grads, all_params, learning_rate=1e-3)\n",
    "\n",
    "\n",
    "f_train = theano.function(inputs=[sym_x],\n",
    "                          outputs=[LL_train, logpx_train, KL_train],\n",
    "                          updates=updates)\n",
    "\n",
    "f_eval = theano.function(inputs=[sym_x],\n",
    "                         outputs=[LL_train, logpx_train, KL_train])\n",
    "\n",
    "f_sample= theano.function(inputs=[sym_z],\n",
    "                         outputs=[mux_sample])\n",
    "\n",
    "f_recon= theano.function(inputs=[sym_x],\n",
    "                         outputs=[mux_eval])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Test the forward pass\n",
    "print  f_train(x_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#plot some samples from the untrained model\n",
    "z = np.random.normal(0,1,size=(100,num_latent_z)).astype('float32')\n",
    "mux_sample = f_sample(z)[0]\n",
    "\n",
    "plot_samples(mux_sample,title='MNIST handwritten samples, untrained model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "num_epochs = 10\n",
    "batch_size = 64\n",
    "num_batch_train = x_train.shape[0] // batch_size\n",
    " \n",
    "LL_train, KL_train, logpx_train = [],[],[]\n",
    "LL_valid, KL_valid, logpx_valid = [],[],[]\n",
    "\n",
    "for e in range(num_epochs):\n",
    "    _LL_train, _KL_train, _logpx_train = [],[],[]\n",
    "    for i in range(num_batch_train):\n",
    "        out = f_train(x_train[batch_size*i:(i+1)*batch_size])\n",
    "        #out = [LL, logpx,KL_qp]\n",
    "        _LL_train += [out[0]]\n",
    "        _logpx_train += [out[1]]\n",
    "        _KL_train += [out[2]]\n",
    "        \n",
    "    LL_train += [np.mean(_LL_train)] \n",
    "    KL_train += [np.mean(_KL_train)] \n",
    "    logpx_train += [np.mean(_logpx_train)] \n",
    "    \n",
    "    out = f_eval(x_valid)\n",
    "    LL_valid += [out[0]]\n",
    "    logpx_valid += [out[1]]\n",
    "    KL_valid += [out[2]]\n",
    "\n",
    "    print \"Epoch %i\\t\"%(e) + \\\n",
    "    \"Train: LL: %0.1f\\tKL %0.1f\\tlogpx: %0.1f\\t\"%(LL_train[-1],KL_train[-1],logpx_train[-1]) + \\\n",
    "    \"Valid: LL: %0.1f\\tKL %0.1f\\tlogpx: %0.1f\"%(LL_valid[-1],KL_valid[-1],logpx_valid[-1])\n",
    "\n",
    "\n",
    "epoch = np.arange(len(LL_train))\n",
    "plt.figure()\n",
    "plt.plot(epoch,LL_train,'r',epoch,LL_valid,'b')\n",
    "plt.legend(['Train LL','Val LL'],loc='best')\n",
    "plt.xlabel('Updates'), plt.ylabel('LL')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#plot some samples from the trained model\n",
    "mux_sample = f_sample(z)[0]\n",
    "plot_samples(mux_sample,title='MNIST handwritten samples, $z\\sim p(z)$')\n",
    "\n",
    "#plot some samples from the trained model\n",
    "mux_recon = f_recon(x_test[:100])[0]\n",
    "plot_samples(mux_recon,title='MNIST handwritten reconstructions, $z\\sim q(z|x)$')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignments\n",
    "Remember that the model defines the probability distribution $p(x,z) = p(x|z)p(z)$. We additionally have the inference network $q(z|x)$ which allows us to infer the latent variables, $z$, for specific input data values $x$.\n",
    "\n",
    "\n",
    "\n",
    "1. Explain how you could sample form the model, which function does this in the code? \n",
    "2. Explain how you could get reconstructions from the model. Remember that you have the inference network $q(z|x)$\n",
    "3. Use the original paper http://arxiv.org/pdf/1312.6114v10.pdf or [this blog](http://blog.shakirm.com/2015/10/machine-learning-trick-of-the-day-4-reparameterisation-tricks/) to explain what the reparameterization trick does. \n",
    "4. The VAE is a probablistic model. We could model $p(x,z,y)$ where $y$ is the label information. How could this model handle semisupervised learning? You can look the papers https://arxiv.org/pdf/1406.5298.pdf or  https://arxiv.org/pdf/1602.05473v4.pdf. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
